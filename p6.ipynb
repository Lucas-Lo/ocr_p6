{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet 6\n",
    "## Description du projet"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import sys, os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import skimage.io as io\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import missingno as msno\n",
    "import seaborn as sns\n",
    "import random\n",
    "import string\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Custom\n",
    "from src.utils import (\n",
    "    count_categories, find_dots, extract_label, tokenize,\n",
    "    word_vectorizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables globales\n",
    "path_to_csv = os.path.join(\"data\", \"flipkart_com-ecommerce_sample_1050.csv\")\n",
    "path_to_images_folder = os.path.join(\"data\", \"images\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = pd.read_csv(path_to_csv)\n",
    "print(df_raw.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw[\"product_category_tree\"].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage des 3 premières données\n",
    "for idx in range(3):\n",
    "    img_path = os.path.join(\"data\", \"images\", df_raw.loc[idx, \"image\"])\n",
    "    img = io.imread(img_path)\n",
    "    print(\"Short description:\", df_raw.loc[idx, \"product_name\"])\n",
    "    print(\"Category tree:\", df_raw.loc[idx, \"product_category_tree\"])\n",
    "    plt.imshow(img)\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Définitions des catégories (labels pour la classif)\n",
    "Le but de cette partie est d'étudier les différents labels que nous pouvons attribuer aux données.\n",
    "La catégorie d'un produit est disponible via un arbre.\n",
    "La question que nous nous poserons est la suivante : Jusqu'à quelle \"profondeur\" pourrons nous aller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count number of sub categories in category_tree\n",
    "print(\"Count number of sub categories min/max for each product\")\n",
    "df_raw[\"nb_of_categories\"] = df_raw[\"product_category_tree\"].apply(\n",
    "    count_categories\n",
    ")\n",
    "print(f\"    There is/are at least  {df_raw['nb_of_categories'].min()} \"\n",
    "      \"sub categories for each product\")\n",
    "print(f\"    There are at max       {df_raw['nb_of_categories'].max()} \"\n",
    "      \"sub categories for each product\\n\")\n",
    "\n",
    "# Extract each different subcategories\n",
    "for pos in range(8):\n",
    "    df_raw[f\"product_category_{pos}\"] = df_raw[\"product_category_tree\"].apply(\n",
    "        extract_label,\n",
    "        args=(pos,)\n",
    "    )\n",
    "\n",
    "print(\"Count all possible combinations:\")\n",
    "# Number of data according to tree depth\n",
    "for i in range(1, 8):\n",
    "    print(f\"    There are {len(df_raw[df_raw['nb_of_categories'] >= i])}\"\n",
    "          f\" data with {i} sub category(ies)\")\n",
    "    df_tmp = df_raw.copy()\n",
    "    df_tmp[\"final_category\"] = df_tmp[\"product_category_0\"]\n",
    "    for k in range(i):\n",
    "        df_tmp[\"final_category\"] = df_tmp[\"final_category\"] +\\\n",
    "            \"_\" + df_tmp[f\"product_category_{k}\"]\n",
    "    print(f\"    -> Which makes {len(df_tmp['final_category'].unique())}\"\n",
    "          \" possible combinations\\n\")\n",
    "    del df_tmp\n",
    "    \n",
    "    \n",
    "print(\"==> To many data is lost if take in a category_tree with at least 4 sub\"\n",
    "      \"\\n    categories. We will create 3 dataframes : df_1, df_2 and df_3\"\n",
    "      \"\\n    corresponding to the category tree depth wanted\")\n",
    "\n",
    "# Creates dataframes\n",
    "# For 1 sub category\n",
    "df1 = df_raw[df_raw[\"nb_of_categories\"] >= 1]\n",
    "df1[\"final_category\"] = df1[\"product_category_0\"]\n",
    "dict_cat_to_label = {\n",
    "    cat:i for (i, cat) in enumerate(df1[\"final_category\"].unique())\n",
    "}\n",
    "df1[\"label\"] = df1[\"final_category\"].apply(lambda x: dict_cat_to_label[x])\n",
    "\n",
    "# For 2 sub categories\n",
    "df2 = df_raw[df_raw[\"nb_of_categories\"] >= 1]\n",
    "df2[\"final_category\"] = df2[\"product_category_0\"] +\\\n",
    "      \"_\" + df2[\"product_category_1\"]\n",
    "dict_cat_to_label = {\n",
    "    cat:i for (i, cat) in enumerate(df2[\"final_category\"].unique())\n",
    "}\n",
    "df2[\"label\"] = df2[\"final_category\"].apply(lambda x: dict_cat_to_label[x])\n",
    "\n",
    "# For 3 sub categories\n",
    "df3 = df_raw[df_raw[\"nb_of_categories\"] >= 1]\n",
    "df3[\"final_category\"] = df3[\"product_category_0\"] +\\\n",
    "      \"_\" + df3[\"product_category_1\"] +\\\n",
    "      \"_\" + df3[\"product_category_2\"]\n",
    "dict_cat_to_label = {\n",
    "    cat:i for (i, cat) in enumerate(df3[\"final_category\"].unique())\n",
    "}\n",
    "df3[\"label\"] = df3[\"final_category\"].apply(lambda x: dict_cat_to_label[x])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Traitement du texte"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III.1. Analyse exploratoire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init parameters\n",
    "df = df1.copy()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "\n",
    "# Tokenize documents\n",
    "tokenized_corpus = [tokenizer.tokenize(doc) for doc in df[\"description\"]]\n",
    "\n",
    "# Lemmatize, lower\n",
    "for i in range(len(tokenized_corpus)):\n",
    "    words = [\n",
    "        lemmatizer.lemmatize(word.lower(), pos=\"v\")\n",
    "        for word in tokenized_corpus[i]\n",
    "    ]\n",
    "    tokenized_corpus[i] = words\n",
    "\n",
    "# Count each words\n",
    "bow = [word for text in tokenized_corpus for word in text]\n",
    "bow = Counter(bow)\n",
    "bow = dict(sorted(bow.items(), key=lambda item: item[1], reverse=True))\n",
    "print(\"20 most occurences:\")\n",
    "for i in range(20):\n",
    "    word = list(bow.keys())[i]\n",
    "    print(f\"{word:<9}: {bow[word]}\")\n",
    "print(\"==> What is rs? Indian Rupees (money)\\n\")\n",
    "\n",
    "# Drop stopwords\n",
    "for i in range(len(tokenized_corpus)):\n",
    "    words = [\n",
    "        word\n",
    "        for word in tokenized_corpus[i]\n",
    "        if word not in stop_words\n",
    "    ]\n",
    "    tokenized_corpus[i] = words\n",
    "\n",
    "# Count each words\n",
    "bow = [word for text in tokenized_corpus for word in text]\n",
    "bow = Counter(bow)\n",
    "bow = dict(sorted(bow.items(), key=lambda item: item[1], reverse=True))\n",
    "print(\"20 most occurences after removing stopwords:\")\n",
    "for i in range(20):\n",
    "    word = list(bow.keys())[i]\n",
    "    print(f\"{word:<9}: {bow[word]}\")\n",
    "\n",
    "print(df[\"description\"].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(doc):\n",
    "    # Init\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    # Tokenize the document\n",
    "    tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "    words = tokenizer.tokenize(doc)\n",
    "    # Lemmatize and remove stop words\n",
    "    words = [\n",
    "        lemmatizer.lemmatize(word.lower(), pos=\"v\")\n",
    "        for word in words\n",
    "        if word.lower() not in stop_words\n",
    "    ]\n",
    "    return words\n",
    "\n",
    "for word in tokenized_corpus[0]:\n",
    "    if word not in preprocess(df[\"description\"].iloc[0]):\n",
    "        print(word)\n",
    "print()\n",
    "for word in preprocess(df[\"description\"].iloc[0]):\n",
    "    if word not in tokenized_corpus[0]:\n",
    "        print(word)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III.2 Extraction de features : BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df3.copy()\n",
    "\n",
    "corpus = df[\"description\"]\n",
    "\n",
    "X, _ = word_vectorizer([tokenize(doc) for doc in corpus], False)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, df[\"label\"], test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = StandardScaler()\n",
    "clf = RandomForestClassifier(min_samples_leaf=2, random_state=77)\n",
    "\n",
    "pipe = Pipeline(steps=[\n",
    "    # (\"preprocessor\", preprocessor),\n",
    "    (\"classifier\", clf)\n",
    "])\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(accuracy_score(y_train, clf.predict(X_train)))\n",
    "print(accuracy_score(y_test, clf.predict(X_test)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B.3 Word tfid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "transformer = TfidfTransformer()\n",
    "transformer.fit_transform(X).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Initialize the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Define your custom stop words list\n",
    "custom_stop_words = set(stopwords.words('english')).union({'specific', 'list', 'of', 'stopwords'})\n",
    "\n",
    "# Custom preprocessing function\n",
    "def preprocess(doc):\n",
    "    # Use regex to clean the text\n",
    "    doc = re.sub(r'\\W', ' ', doc)\n",
    "    # Tokenize the document\n",
    "    words = word_tokenize(doc)\n",
    "    # Lemmatize and remove stop words\n",
    "    words = [lemmatizer.lemmatize(word.lower()) for word in words if word.lower() not in custom_stop_words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Example corpus\n",
    "corpus = [\n",
    "    'Cats are running faster than dogs.',\n",
    "    'The cat ran faster than the dog.'\n",
    "]\n",
    "\n",
    "# Preprocess the corpus\n",
    "preprocessed_corpus = [preprocess(doc) for doc in corpus]\n",
    "\n",
    "# Create CountVectorizer and TfidfVectorizer\n",
    "count_vectorizer = CountVectorizer()\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the preprocessed corpus\n",
    "X_count = count_vectorizer.fit_transform(preprocessed_corpus)\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(preprocessed_corpus)\n",
    "\n",
    "# Print the results\n",
    "print(\"Count Vectorizer Feature Names:\", count_vectorizer.get_feature_names_out())\n",
    "print(\"Count Vectorizer Array:\\n\", X_count.toarray())\n",
    "print(\"TF-IDF Vectorizer Feature Names:\", tfidf_vectorizer.get_feature_names_out())\n",
    "print(\"TF-IDF Vectorizer Array:\\n\", X_tfidf.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Custom preprocessing function\n",
    "def lemmatize_text(text):\n",
    "    words = word_tokenize(text)\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return ' '.join(lemmatized_words)\n",
    "\n",
    "# Create the CountVectorizer with the custom preprocessing function\n",
    "vectorizer = CountVectorizer(preprocessor=lemmatize_text)\n",
    "\n",
    "# Example usage\n",
    "corpus = [\n",
    "    'Cats are running faster than dogs.',\n",
    "    'The cat ran faster than the dog.'\n",
    "]\n",
    "\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(X.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ocr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "49f5a819dfb3ad0ef33c81c72d060d708f9052ca1509dc70ad6cc32112032782"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
